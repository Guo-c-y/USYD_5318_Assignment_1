{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 Assignment 1: Rice Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Group number: ...\n",
    "##### Student 1 SID: ...\n",
    "##### Student 2 SID: ...  \n",
    "##### Student 3 SID: ... \n",
    "##### Student 4 SID: ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    train_test_split\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    BaggingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore future warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the rice dataset: rice-final2.csv\n",
    "data = pd.read_csv(\"rice-final2.csv\", na_values=\"?\")\n",
    "# data = pd.read_csv(\"test-before.csv\", na_values=\"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first ten rows of pre-processed dataset to 4 decimal places as per assignment spec\n",
    "# A function is provided to assist\n",
    "\n",
    "def print_data(X, y, n_rows=10):\n",
    "    \"\"\"Takes a numpy data array and target and prints the first ten rows.\n",
    "    \n",
    "    Arguments:\n",
    "        X: numpy array of shape (n_examples, n_features)\n",
    "        y: numpy array of shape (n_examples)\n",
    "        n_rows: numpy of rows to print\n",
    "    \"\"\"\n",
    "    for example_num in range(n_rows):\n",
    "        for feature in X[example_num]:\n",
    "            print(\"{:.4f}\".format(feature), end=\",\")\n",
    "\n",
    "        if example_num == len(X)-1:\n",
    "            print(y[example_num],end=\"\")\n",
    "        else:\n",
    "            print(y[example_num])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4628,0.5406,0.5113,0.4803,0.7380,0.4699,0.1196,1\n",
      "0.4900,0.5547,0.5266,0.5018,0.7319,0.4926,0.8030,1\n",
      "0.6109,0.6847,0.6707,0.5409,0.8032,0.6253,0.1185,0\n",
      "0.6466,0.6930,0.6677,0.5961,0.7601,0.6467,0.2669,0\n",
      "0.6712,0.6233,0.4755,0.8293,0.3721,0.6803,0.4211,1\n",
      "0.2634,0.2932,0.2414,0.4127,0.5521,0.2752,0.2825,1\n",
      "0.8175,0.9501,0.9515,0.5925,0.9245,0.8162,0.0000,0\n",
      "0.3174,0.3588,0.3601,0.3908,0.6921,0.3261,0.8510,1\n",
      "0.3130,0.3050,0.2150,0.5189,0.3974,0.3159,0.4570,1\n",
      "0.5120,0.5237,0.4409,0.6235,0.5460,0.5111,0.3155,1\n"
     ]
    }
   ],
   "source": [
    "# Pre-process dataset\n",
    "\n",
    "# 1. Separate features and labels\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# 2. Handle missing values: Replace missing values with column mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# 3. Normalize features: Scale all features into range [0, 1]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# 4. Encode class labels: Convert class1 -> 0, class2 -> 1\n",
    "y = np.where(y == \"class1\", 0, 1)\n",
    "\n",
    "# 5. Printing function as per assignment spec\n",
    "print_data(X, y, n_rows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processed dataset saved as rice-final2-preprocessed.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Combine processed features and labels ---\n",
    "processed_df = pd.DataFrame(X, columns=data.drop(columns=['class']).columns)\n",
    "processed_df['class'] = y  # add processed class labels\n",
    "\n",
    "# --- Save pre-processed dataset ---\n",
    "processed_df.to_csv(\"rice-final2-preprocessed.csv\", index=False)\n",
    "print(\"Pre-processed dataset saved as rice-final2-preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Cross-validation without parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the 10 fold stratified cross-validation\n",
    "cvKFold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# The stratified folds from cvKFold should be provided to the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "def logregClassifier(X, y):\n",
    "    clf = LogisticRegression(random_state=0, max_iter=1000)\n",
    "    scores = cross_val_score(clf, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Na√Øve Bayes\n",
    "def nbClassifier(X, y):\n",
    "    clf = GaussianNB()\n",
    "    scores = cross_val_score(clf, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "def dtClassifier(X, y):\n",
    "    clf = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "    scores = cross_val_score(clf, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensembles: Bagging, Ada Boost and Gradient Boosting\n",
    "def bagDTClassifier(X, y, n_estimators, max_samples, max_depth):\n",
    "    base_clf = DecisionTreeClassifier(criterion='entropy', max_depth=max_depth, random_state=0)\n",
    "    clf = BaggingClassifier(estimator=base_clf,\n",
    "                            n_estimators=n_estimators,\n",
    "                            max_samples=max_samples,\n",
    "                            random_state=0)\n",
    "    scores = cross_val_score(clf, X, y, cv=cvKFold)\n",
    "    return scores.mean()\n",
    "\n",
    "def adaDTClassifier(X, y, n_estimators, learning_rate, max_depth):\n",
    "    base_clf = DecisionTreeClassifier(criterion='entropy', max_depth=max_depth, random_state=0)\n",
    "    clf = AdaBoostClassifier(estimator=base_clf,\n",
    "                             n_estimators=n_estimators,\n",
    "                             learning_rate=learning_rate,\n",
    "                             random_state=0)\n",
    "    scores = cross_val_score(clf, X, y, cv=cvKFold)\n",
    "    return scores.mean()\n",
    "\n",
    "def gbClassifier(X, y, n_estimators, learning_rate):\n",
    "    clf = GradientBoostingClassifier(n_estimators=n_estimators,\n",
    "                                     learning_rate=learning_rate,\n",
    "                                     random_state=0)\n",
    "    scores = cross_val_score(clf, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogR average cross-validation accuracy: 0.6700\n",
      "NB average cross-validation accuracy: 0.6555\n",
      "DT average cross-validation accuracy: 0.7750\n",
      "Bagging average cross-validation accuracy: 0.7514\n",
      "AdaBoost average cross-validation accuracy: 0.7224\n",
      "GB average cross-validation accuracy: 0.7464\n"
     ]
    }
   ],
   "source": [
    "# Parameters for Part 1:\n",
    "\n",
    "#Bagging\n",
    "bag_n_estimators = 50\n",
    "bag_max_samples = 100\n",
    "bag_max_depth = 5\n",
    "\n",
    "#AdaBoost\n",
    "ada_n_estimators = 50\n",
    "ada_learning_rate = 0.5\n",
    "ada_bag_max_depth = 5\n",
    "\n",
    "#GB\n",
    "gb_n_estimators = 50\n",
    "gb_learning_rate = 0.5\n",
    "\n",
    "# Print results for each classifier in part 1 to 4 decimal places here:\n",
    "print(\"LogR average cross-validation accuracy: {:.4f}\".format(logregClassifier(X, y)))\n",
    "print(\"NB average cross-validation accuracy: {:.4f}\".format(nbClassifier(X, y)))\n",
    "print(\"DT average cross-validation accuracy: {:.4f}\".format(dtClassifier(X, y)))\n",
    "print(\"Bagging average cross-validation accuracy: {:.4f}\".format(bagDTClassifier(X, y, bag_n_estimators, bag_max_samples, bag_max_depth)))\n",
    "print(\"AdaBoost average cross-validation accuracy: {:.4f}\".format(adaDTClassifier(X, y, ada_n_estimators, ada_learning_rate, ada_bag_max_depth)))\n",
    "print(\"GB average cross-validation accuracy: {:.4f}\".format(gbClassifier(X, y, gb_n_estimators, gb_learning_rate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Cross-validation with parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "k = [1, 3, 5, 7]\n",
    "p = [1, 2]\n",
    "\n",
    "\n",
    "def bestKNNClassifier(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, stratify=y, random_state=0\n",
    "    )\n",
    "\n",
    "    param_grid = {\"n_neighbors\": k, \"p\": p}\n",
    "    grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=cvKFold)\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid.best_params_\n",
    "    best_cv_score = grid.best_score_\n",
    "    test_accuracy = accuracy_score(y_test, grid.predict(X_test))\n",
    "\n",
    "    return best_params, best_cv_score, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# You should use RandomForestClassifier from sklearn.ensemble with information gain and max_features set to ‚Äòsqrt‚Äô.\n",
    "n_estimators = [10, 30, 60, 100]\n",
    "max_leaf_nodes = [6, 12]\n",
    "\n",
    "def bestRFClassifier(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, stratify=y, random_state=0\n",
    "    )\n",
    "\n",
    "    param_grid = {\"n_estimators\": n_estimators, \"max_leaf_nodes\": max_leaf_nodes}\n",
    "    grid = GridSearchCV(\n",
    "        RandomForestClassifier(\n",
    "            criterion=\"entropy\", max_features=\"sqrt\", random_state=0\n",
    "        ),\n",
    "        param_grid,\n",
    "        cv=cvKFold,\n",
    "    )\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid.best_params_\n",
    "    best_cv_score = grid.best_score_\n",
    "    y_pred = grid.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    f1_weighted = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "    return best_params, best_cv_score, test_accuracy, f1_macro, f1_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN best k: 7\n",
      "KNN best p: 1\n",
      "KNN cross-validation accuracy: 0.7038\n",
      "KNN test set accuracy: 0.6349\n",
      "\n",
      "RF best n_estimators: 100\n",
      "RF best max_leaf_nodes: 6\n",
      "RF cross-validation accuracy: 0.8071\n",
      "RF test set accuracy: 0.6667\n",
      "RF test set macro average F1: 0.6437\n",
      "RF test set weighted average F1: 0.6566\n"
     ]
    }
   ],
   "source": [
    "# Perform Grid Search with 10-fold stratified cross-validation (GridSearchCV in sklearn). \n",
    "cvKFold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "# The stratified folds from cvKFold should be provided to GridSearchV\n",
    "\n",
    "# This should include using train_test_split from sklearn.model_selection with stratification and random_state=0\n",
    "# Print results for each classifier here. All results should be printed to 4 decimal places except for\n",
    "# \"k\", \"p\", n_estimators\" and \"max_leaf_nodes\" which should be printed as integers.\n",
    "# KNN\n",
    "knn_params, knn_cv_acc, knn_test_acc = bestKNNClassifier(X, y)\n",
    "print(\"KNN best k: {}\".format(knn_params[\"n_neighbors\"]))\n",
    "print(\"KNN best p: {}\".format(knn_params[\"p\"]))\n",
    "print(\"KNN cross-validation accuracy: {:.4f}\".format(knn_cv_acc))\n",
    "print(\"KNN test set accuracy: {:.4f}\".format(knn_test_acc))\n",
    "\n",
    "print()\n",
    "\n",
    "# RF\n",
    "rf_params, rf_cv_acc, rf_test_acc, rf_f1_macro, rf_f1_weighted = bestRFClassifier(X, y)\n",
    "print(\"RF best n_estimators: {}\".format(rf_params[\"n_estimators\"]))\n",
    "print(\"RF best max_leaf_nodes: {}\".format(rf_params[\"max_leaf_nodes\"]))\n",
    "print(\"RF cross-validation accuracy: {:.4f}\".format(rf_cv_acc))\n",
    "print(\"RF test set accuracy: {:.4f}\".format(rf_test_acc))\n",
    "print(\"RF test set macro average F1: {:.4f}\".format(rf_f1_macro))\n",
    "print(\"RF test set weighted average F1: {:.4f}\".format(rf_f1_weighted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
